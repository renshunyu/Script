./kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group aus-group --zookeeper 10.1.198.61:5181/consumers

select hour(operate_time),count(1) from iap_app_log_import_minute t where day(operate_time)=16 and t.import_time like '2016-12-%' group by hour(operate_time) order by hour(operate_time)

github.com/datafoundry/training


https://github.com/boot2docker/windows-installer/releases/download/v1.8.0/docker-install.exe



https://s10d.storage.yandex.net/rdisk/298936885f85c44263ceeb8d5115061b1a57905840d11317b1e18beff9866d64/586f837d/jEureRSpFNh_apPkYXRiEbDkDPeu5Pequ1x2x7rAss1ytX5wlJUVngyKFVaEkdmzxtMjoq7FCj7rT2EIZwHWMQ==?uid=0&filename=Chromedriver.zip&disposition=attachment&hash=3CuUzVwzoPngn5ni7J8Py1FXq7xEJUoA4qLJo85CyiM%3D%3A&limit=1&content_type=application%2Fx-zip-compressed&fsize=2398137&hid=fa0499d064d79ae880ea0ba05a7d64a1&media_type=compressed&tknv=v2&rtoken=xDKrCi7gkOY6&force_default=no&ycrid=na-e43e9baa155c213653a37c4a9daba488-downloader8g&ts=5456b901a1940&s=e4655db1d8de60e4ecf76cdc047dd38ee02243ce95ec0efd5de43c5c542eafd7&bp=/45/7/data-0.9:17485928220:2398137&pb=U2FsdGVkX1-UYrQTfKWyblzWoXfBWG1R65sdXI2SSMGTT2Hdzndgf4Tteb2Lm9z2g8oNWnXB8-pm-tdhSGE1CkEbc2GtGQgl9ggs90hMrdU=


bin/kafka-console-consumer.sh --zookeeper 10.1.198.61:5181 --topic tcp-xml --from-beginning


sh kafka-console-consumer.sh --zookeeper 10.1.198.61:5181 --topic tcp-xml --from-beginning|grep 1-SIMS-1-10008|grep 2016-12


/ai4a30/aisia1.0/产品发布/aisiav04r00c00_20161111/安装包/iap.war


java -jar "selenium-server-standalone-2.52.0.jar" -Dwebdriver.chrome.bin=".\chromedrivesr.exe"

ClassPathXmlApplicationContext actx=new ClassPathXmlApplicationContext("main-spring.xml");
File fi = (File) actx.getBean("filepath");


imp audit30testa/audit30testa#@iap file=oridev_cmdlog_201701.dmp full=y log=1.log

pip install http://wxpython.org/Phoenix/snapshot-builds/ wxPython_Phoenix

./zookeeper-server-cleanup /var/lib/zookeeper/version-2  2

src/main/resource

sudo chgrp -R zookeeper /app/lib/zookeeper
sudo chown -R zookeeper /app/lib/zookeeper

cat xml.out |grep LOG4A|awk -F '[<>]' '{a[$47]++}END{for (i in a) {print $i}}'
cat xml.out |grep LOG4A|awk -F '[<>]' '{print $47}'
cat xml.out |grep LOG4A|awk -F '[<>]' '{j=NR;print $0 >> j".xml"}'
cat ../xml.out |grep LOG4A|awk -F '[<>]' '{print $0 >> $27".txt"}'
rm $(ls |grep 20072|awk '{if (NR!=1){print $1}}')
cat xml.out |grep LOG4A|awk -F '[<>]' '{a[$27]++}END{for (i in a) {print i"  "a[i]}}'
cat xml.out |grep LOG4A|awk -F '[<>]' '{a[$47]++}END{for (i in a) {print i"  "a[i]}}'




kill -9 $(ps -ef |grep GatherSend|grep -v grep|awk '{print $2}')
sh /home/aiuap/autobuild/gather_serverbuild.sh 
sh /home/aiuap/autobuild/gather_server_tcp-xml_deploy.sh
sleep 5
sh start.sh uac record 30
tail -f /home/aiuap30/gather30/ap_gather_server_tcp-xml/ap_gather_server_tcp-xml/log/ap.log


kill -9 $(ps -ef |grep ap_gather_server_tcp-xml|grep -v grep|awk '{print $2}')
csh start ap_gather_server_tcp-xml


curl -XGET 10.19.19.40:8990/_cluster/health -u kibanaserver:Kibanaserver4A@2017




cat * |grep UAP|awk -F 'SUB_ACCOUNT_NAME' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'SERVER_ADDRESS' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'OPERATE_RESULT' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'OP_TYPE_ID' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'RESOURCE_CODE' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'MAIN_ACCOUNT_NAME' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'SERVER_PORT' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'CLIENT_NETWORK_ADDRESS' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'OPERATE_TIME' '{print $2}'|sed 's/>//g'|sed 's/<\///g'
cat * |grep UAP|awk -F 'OPERATE_CONTENT' '{print $2}'|sed 's/>//g'|sed 's/<\///g'


find audittest |grep no |awk '{print "csh logsendtool.sh uac "$1}'
keyihelvmengduijie
xitongziyuanjiangkong
zhichiweixin
rizhi  shiijanzhuangfa


es删除历史数据
POST app-log/app-log-2016-11/_delete_by_query?conflicts=proceed
{
  "query": {
    "match_all": {}
  }
}

export JAVA_HOME=/usr/java/jdk1.8.0_121
export CLASSPATH=$(echo $CLASSPATH|sed 's/jdk1.6.0_31/jdk1.8.0_121/g')
export PATH=$(echo $PATH:$HOME/bin:/home/aiuap/tools/apache-ant-1.7.1/bin|sed 's/jdk1.6.0_31/jdk1.8.0_121/g')


<CLIENT_BOIS_SERIAL>1234567890</CLIENT_BOIS_SERIAL><HARD_DISK_SERIAL>0987654321</HARD_DISK_SERIAL>
sed -i "s/<ROOT><LOG4A>/<ROOT><LOG4A><CLIENT_BOIS_SERIAL>1234567890<\/CLIENT_BOIS_SERIAL><HARD_DISK_SERIAL>0987654321<\/HARD_DISK_SERIAL>/g"  主帐号*

vi /etc/security/limits.d/90-nproc.conf 
修改如下内容：
* soft nproc 1024
#修改为
* soft nproc 10240


alter database add logfile group 4 ('/aifs01/oracle11g/oradata/iap/redo04.log') size 1024M;
alter database add logfile group 5 ('/aifs01/oracle11g/oradata/iap/redo05.log') size 1024M;
alter database add logfile group 6 ('/aifs01/oracle11g/oradata/iap/redo06.log') size 1024M;

echo 'select count(*) from v$process'|sqlplus audit30testa/audit30testa#@ai4a

echo 'select count(*) from v$process;'|sqlplus audit30testa/audit30testa#@iap |grep '    '|awk '{print $NF}'

mvn install:install-file -Dfile=ap-simulator-1.0.jar -DgroupId=com.asiainfo -DartifactId=ap-simulator -Dversion=1.0 -Dpackaging=jar
mvn install:install-file -Dfile=ap-commons-1.0.jar -DgroupId=com.asiainfo -DartifactId=ap-commons -Dversion=1.0 -Dpackaging=jar
mvn install:install-file -Dfile=ojdbc6-11.2.0.3.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar

cat `ls -t|grep err.log|awk 'NR<3'`|grep "`date '+%F %H:%M' -d '2017-07-22 14:20:02,180'`"

ls -t|grep err.log|awk 'NR<3'

sed "s\value=\".*?\"\value=\"`date "+%S %M %H * * ?" -d '60 second'`\"\g" iap-spring-report.xml 
sed "30 s/value=\".*\"/value=\"`date "+%S %M %H * * ?" -d '60 second'`\"/g" iap-spring-report.xml
sqlplus audit30testa/audit30testa#@10.15.42.37:1521/iap


echo 'truncate table MODEL_MIDDLE_DETAIL_RESULT;'|sqlplus audit30testa/audit30testa#@10.15.42.37:1521/iap
echo "truncate table MODEL_MIDDLE_DETAIL_RESULT;
update iap_report_schedule set his_data_analysis_finish_time='' where report_id = 'ModelAudit ';
commit;"|sqlplus audit30testa/audit30testa#@10.15.42.37:1521/iap
echo 'use aiiap;
truncate table MODEL_MIDDLE_DETAIL_RESULT;' |hive

echo 'use aiiap;
show partitions MODEL_MIDDLE_DETAIL_RESULT;' |hive|grep operate_day|awk -F= '{print "use aiiap;\nalter table MODEL_MIDDLE_DETAIL_RESULT drop partition("$1"='\''"$2"'\'');" }'|hive

Insert into IAP_APP_LOG_ANALUSE_NORMAL_RESULT select  '10000048',t.main_acct_name ,'1-AIUAP-80005',t.operate_type_name,'70' from  aiiaptestc.iap_app_log_operate_day t where t.main_acct_id = '10000048' and t.operate_type_id = '1-AIUAP-80005' and  t.operate_day='2017-08-02' limit 1;
Insert into IAP_APP_LOG_ANALUSE_NORMAL_RESULT select  '10000038',t.main_acct_name ,'1-AIUAP-20427',t.operate_type_name,'70' from  aiiaptestc.iap_app_log_operate_day t where t.main_acct_id = '10000038' and t.operate_type_id = '1-AIUAP-20427' and  t.operate_day='2017-07-19' limit 1;
Insert into IAP_APP_LOG_ANALUSE_NORMAL_RESULT select t.main_acct_id,t.main_acct_name,t.operate_type_id,t.operate_type_name,'70' as num from  aiiaptestc.iap_app_log_operate_day t where t.operate_day = '2017-07-20' and t.main_acct_name<>'NULL' and t.operate_type_name not in ('主帐号加锁','主帐号新建','主帐号解锁') group by t.main_acct_id,t.main_acct_name,t.operate_type_id,t.operate_type_name;

select * from  iap_app_log_operate_day t where t.main_acct_id = '10000048' and t.operate_type_id = '1-AIUAP-80005' and  t.operate_day='2017-08-02';
select * from  iap_app_log_operate_day t where t.main_acct_id = '10000038' and t.operate_type_id = '1-AIUAP-20427' and  t.operate_day='2017-07-20';

pwdx `jps|grep ReportApp|awk '{print $1}'`
sudo netstat -anop |grep 1521|awk '{a[$7]++}END{for(i in a){print i" "a[i]}}'
hadoop fs -du -h `hadoop fs -du -h /user/hive/warehouse/aiiap.db/iap_app_log_operate_day/|grep 2017-07|awk '{print $5}'`|grep K|awk '{print $NF}'|awk -F/ '{a[$7]++}END{for(i in a){print i" "a[i]}}'
grep -n ERROR catalina.out |awk -F: '{a[$2]++}END{for( i in a ){print i" "a[i]}}'|sort

curl -XGET -u kibanaserver:Kibanaserver4A@2017 http://10.1.198.55:8990/app-log-v1/_count
hadoop fs -ls /user/hive/warehouse/aiiap.db/iap_app_log_operate_day|grep =|awk -F/ '{print "use aiiap;\nalter table iap_app_log_operate_day add partition("$NF"'\'');"}'|sed "s/=/='/g"|hive
hadoop fs -lsr  /user/hive/warehouse/aiiaptestc.db/|grep =|awk -F/ '{print "alter table "$6" add partition("$NF"'\'');"}'|sed "s/=/='/g"|grep =
hadoop fs -lsr  /user/hive/warehouse/aiiaptestc.db/|grep =|awk -F= '(NF>1 && !match($NF,"/")){print $0}'|awk -F/ '{if(NF==7){print "alter table "$6" add partition("$7"'\'');"};if(NF==8){print "alter table "$6" add partition("$7"'\'',"$8"'\'');"}}'|sed "s/=/='/g"
hadoop fs -lsr  /user/hive/warehouse/aiiaptestc.db/|grep =|awk -F= '(NF>1 && !match($NF,"/")){print $0}'|awk -F/ '{if(NF==7){print "use aiiaptestc;\nalter table "$6" add partition("$7"'\'');"};if(NF==8){print "use aiiaptestc;\nalter table "$6" add partition("$7"'\'',"$8"'\'');"}}'|sed "s/=/='/g"|hive

for i in $(ls /proc | grep "^[0-9]" | awk '$0>100'); do awk '/Swap:/{a=a+$2}END{print '"$i"',a/1024"M"}' /proc/$i/smaps;done| sort -k2nr | head -n 20|awk '{print $1}'|xargs pwdx

SELECT TO_CHAR(TO_DATE('2009-04' || '-1', 'YYYY-MM-DD') + ROWNUM - 1,
               'YYYYMMDD') D1,TO_CHAR(TO_DATE('2009-04' || '-1', 'YYYY-MM-DD') + ROWNUM,
               'YYYY-MM-DD hh24:mi:ss') D2
  FROM DUAL
CONNECT BY ROWNUM <=32 

SELECT 'partition IAP_ORI_'||TO_CHAR(TO_DATE('2017-12' || '-1', 'YYYY-MM-DD') + ROWNUM - 1,
               'YYYYMMDD')||' values less than (TIMESTAMP'''||TO_CHAR(TO_DATE('2017-12' || '-1', 'YYYY-MM-DD') + ROWNUM,
               'YYYY-MM-DD hh24:mi:ss')||'''),'
  FROM DUAL
CONNECT BY ROWNUM <=32 ;

SELECT TO_CHAR(ADD_MONTHS(TO_DATE('2009-03', 'YYYY-MM'), ROWNUM - 1),
               'YYYY-MM') DAY_ID
  FROM DUAL
CONNECT BY ROWNUM <=
           months_between(to_date('2010-03', 'yyyy-mm'),
                          to_date('2009-03', 'yyyy-mm')) + 1


echo 'use aiiaptestc;
show partitions iap_app_log_import_minute;'|hive |grep 2017-09|awk -F = '{print "use aiiaptestc;\nalter table iap_app_log_import_minute drop partition ("$1"='\''"$2"'\'');"}'|hive

#oracle延迟函数
select dbms_pipe.receive_message('RDS', 10)   from dual;

project = SSSIA AND issuetype = 故障 AND assignee in (lusm, wangsz, tanrq, yutong3, zhangjy11, rensy, zhanghl8, liufd)
project = SSSIA AND issuetype = 故障 AND status in (Open, 已上线, 已发布, 待发布, 待测试, 测试中, 定位中, 待修复, 故障修复中, 已提交) AND assignee not in (lusm, wangsz, tanrq, yutong3, zhangjy11, rensy, zhanghl8, liufd)

for i in 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29; 

do 

echo $i;

sudo dd if=/dev/zero of=/usr/swap_file$i bs=2G count=1;

sudo chmod 600 /usr/swap_file$i;

sudo mkswap /usr/swap_file$i;

sudo swapon /usr/swap_file$i;

#/usr/swap_file10 swap swap defaults 0 0

done

select decode(data_type,
              'TIMESTAMP(6)',
              'to_char(' || t.COLUMN_NAME ||
              ',''yyyy-mm-dd hh24:mi:ss'') as ' || t.COLUMN_NAME || ',',
              'DATE',
              'to_char(' || t.COLUMN_NAME ||
              ',''yyyy-mm-dd hh24:mi:ss'') as ' || t.COLUMN_NAME || ',',
              t.COLUMN_NAME||',') as COLUMN_NAME
  from user_tab_columns t
 where t.TABLE_NAME = 'UAP_MAIN_ACCT';

select 'select '||wm_concat(COLUMN_NAME) || ' from ' || 'UAP_MAIN_ACCT'
  from (select decode(data_type,
                      'TIMESTAMP(6)',
                      'to_char(' || t.COLUMN_NAME ||
                      ',''yyyy-mm-dd hh24:mi:ss'') as ' || t.COLUMN_NAME,
                      'DATE',
                      'to_char(' || t.COLUMN_NAME ||
                      ',''yyyy-mm-dd hh24:mi:ss'') as ' || t.COLUMN_NAME,
                      t.COLUMN_NAME) as COLUMN_NAME
          from user_tab_columns t
         where t.TABLE_NAME = 'UAP_MAIN_ACCT');

select (select 'select ' || wm_concat(decode(data_type,
                                             'TIMESTAMP(6)',
                                             'to_char(' || t.COLUMN_NAME ||
                                             ',''yyyy-mm-dd hh24:mi:ss'') as ' ||
                                             t.COLUMN_NAME,
                                             'DATE',
                                             'to_char(' || t.COLUMN_NAME ||
                                             ',''yyyy-mm-dd hh24:mi:ss'') as ' ||
                                             t.COLUMN_NAME,
                                             t.COLUMN_NAME)) || ' from ' ||
               ut.table_name
          from user_tab_columns t
         where t.TABLE_NAME = ut.table_name)
  from user_tables ut;

 watch -n 5 " echo 'use iaptest;

select count(1) from IAP_ORI_APP_LOG_201705;'|mysql -u root -p'2wsx#EDC'"


awk '$1=="INSERT" || $1=="insert"{print $3}' 07_全部表数据.sql 



svn cp -m "create branch" http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/program/iap http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/release/aisiav04r04c02_20180509/iap
PJ=data-reporting&&svn cp -m "create branch" http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/program/$PJ http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/release/aisiav04r04c02_20180509/$PJ
svn -r133313:134535 log http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/program/iap
svn diff -r134866:134581 --summarize http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30
svn log http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/release/aisiav04r20c00-pacth03_20171128/iap --stop-on-copy -q
java -jar hudson-cli.jar -s http://10.1.198.53:9080/hudson build iap -p sadf=ewdwq

ncftpput -u 'ai\rensy' -p '3edc@WSX' -P 21 -m -R 10.1.252.239 ./ai4a30/aisia1.0/临时开发版/ 

svn info `ls |grep -v 'iap_aus_zjy'` |grep 'URL:'
svn log http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/release/aisiav04r04c03_20180531 -v |grep 'products'|grep -v ' (from '

curl -X GET  http://10.1.198.53:10011/lastTaskInfo
curl -X GET  http://10.1.198.53:10011/info
curl -X GET  http://10.1.198.53:10011/cycleInfo
echo ",{\"time\":`date +%s`,\"response\":`curl -X GET  http://10.1.198.53:10011/lastTaskInfo`}"


js-yaml /home/aiuap30/data-reporting/data-reporting-0.0.1-SNAPSHOT/config/application.yml.new |sed 's/-/_/g'|jq .upload_task
js-yaml /home/aiuap30/data-reporting/data-reporting-0.0.1-SNAPSHOT/config/application.yml.new |sed 's/-/_/g'|jq 'has("name")
for i in {4..4}; do sed "s/base/base$i/g" huizhi.sh >huizhi$i.sh;sed -i "s/upload/upload$i/g" huizhi$i.sh; done
for i in {6..36}; do nohup sh huizhi$i.sh &  done
sed -i 's/ enabled: false/ enabled: true/g'  `find . -name application.yml`


查询Oracle正在执行和执行过的SQL语句
--查询Oracle正在执行的sql语句及执行该语句的用户

SELECT b.sid oracleID,
       b.username 登录Oracle用户名,
       b.serial#,
       spid 操作系统ID,
       paddr,
       sql_text 正在执行的SQL,
       b.machine 计算机名
FROM v$process a, v$session b, v$sqlarea c
WHERE a.addr = b.paddr
   AND b.sql_hash_value = c.hash_value

 

 

--查看正在执行sql的发起者的发放程序

SELECT OSUSER 电脑登录身份,
       PROGRAM 发起请求的程序,
       USERNAME 登录系统的用户名,
       SCHEMANAME,
       B.Cpu_Time 花费cpu的时间,
       STATUS,
       B.SQL_TEXT 执行的sql
FROM V$SESSION A
LEFT JOIN V$SQL B ON A.SQL_ADDRESS = B.ADDRESS
                   AND A.SQL_HASH_VALUE = B.HASH_VALUE
ORDER BY b.cpu_time DESC

 

 

--查出oracle当前的被锁对象

SELECT l.session_id sid,
       s.serial#,
       l.locked_mode 锁模式,
       l.oracle_username 登录用户,
       l.os_user_name 登录机器用户名,
       s.machine 机器名,
       s.terminal 终端用户名,
       o.object_name 被锁对象名,
       s.logon_time 登录数据库时间
FROM v$locked_object l, all_objects o, v$session s
WHERE l.object_id = o.object_id
   AND l.session_id = s.sid
ORDER BY sid, s.serial#;

 

 

--kill掉当前的锁对象可以为

alter system kill session 'sid， s.serial#‘;

--查询oracle关键字
select * from v$reserved_words order by keyword asc;

svn log http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/program/data-reporting -r139842:138652 |grep -v '\-\-\-\-\-\-\-\-\-\-\-\-'|grep  '^r'|grep 'line$'|awk -F'|' '{print "<tr><td class=tablerow>"$1"</td><td class=tablerow>"$3"</td><td class=tablerow>"$2"</td><td class=tablerow></td></tr>"}'
svn log http://10.1.198.30/svn/UAPPROGRAM/products/ailkiap30/program/data-reporting -r139842:138652 |grep -v '\-\-\-\-\-\-\-\-\-\-\-\-'|grep -v '^$'|awk -F'|' '{if(NF==4){if(NR!=1){print "</tr>"}print "  <tr><td class=tablerow>"$1"</td><td class=tablerow>"$3"</td><td class=tablerow>"$2"</td>"}else{print "<td class=tablerow>"$0"</td>"}}' >>duibi.html


md5sum `find . -type f` |wc -l
md5sum `find . -type f` |awk '{a[$1]++}{for(i in a ){print i}}'
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){print i}}'
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){print i}}'|wc -l
md5sum `find . -type f` |wc -l
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){if (a[i] > 1){print i}}}'
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){if (a[i] > 1){print i""a[i]}}}'
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){if (a[i] > 1){print i""$a[i]}}}'
md5sum `find . -type f` |awk '{a[$1]++}END{for(i in a ){if (a[i] > 1){print i}}}'
awk '{a[$0]++}END{for(i in a){if(a[i]==1){print i}}}' a.txt b.txt |awk '{print $2" "$1}'
集合B减去集合A，即a文件里面存在b文件里面不存在的
awk 'ARGIND==1{a[$0]=$2;next}{if (!($0 in a)){print $2}}' a.txt b.txt
集合A减去集合B，即b文件里面存在a文件里面不存在的
awk 'ARGIND==1{a[$0]=$2;next}{if (!($0 in a)){print $2}}' b.txt a.txt
awk自定义函数
awk 'BEGIN{print "asd"}function getpath(fpath){split(fpath,a,"a");str="";for (i=1;i<length(a);i++ ){str= str a[i]"/"}return str}BEGIN{print getpath("sadaverfssdfasdfr")}'

估算pi
nohup hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.4.3.jar pi 1000 100000000 &
hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.4.3.jar bbp 1 10000 10 hahsda

启动flume
nohup ./flume-ng agent --conf ../conf --conf-file ../conf/flume.conf -name agent -Dflume.root.logger=INFO,console &

启动kafka
nohup sh kafka-server-start.sh ../config/server.properties &
查看kafka数据
sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic flumeTopic --from-beginning
nohup sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic flumeTopic --from-beginning &
查询kafka的topic
sh kafka-topics.sh  --list --zookeeper localhost:2181
删除kafka的topic
sh kafka-topics.sh  --delete  --zookeeper localhost:2181 --topic flumeTopic
监控当前目录的进程
top -b -n 1 -d 1 -p $(pwdx `jps -q`|grep `pwd`|awk -F: '{print $1}') |grep -E java
top -b -n 720 -d 1 -p $(pwdx `jps -q`|grep `pwd`|awk -F: '{print $1}') |awk '{print systime()" "$0}'|grep java>jiangkong.txt
指定输出分割符
awk -v OFS="zhibiaofu" '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13}' jiangkong.txt


启动filebeat
nohup ./filebeat -e -c filebeat.yml &

监控beat进程
top -b -n 1 -d 1 -p $(ps -ef |grep filebeat|grep -v grep|awk '{print $2}') |grep -E filebeat
top -b -n 720 -d 1 -p $(ps -ef |grep filebeat|grep -v grep|awk '{print $2}') |awk '{print systime()" "$0}'|grep filebeat>>jiangkong.txt

安装kafka插件
/opt/td-agent/embedded/bin/fluent-gem install fluent-plugin-kafka

ftp下载文件
ncftpget -u 'ai\rensy' -p '2wsx!QAZ' -R  10.1.252.239 . /ai4a30/aisia1.0/产品发布/aisiamv04r06c00_20190111/db脚本/audit

for i in `ls`; do head -n 1 $i; done|awk '{a[$0]=NR}END{for (i in a) {print i" "a[i]}}'

打war包是tmp空间不足，指定临时目录
 mvn package -Dmaven.test.skip=true -Djava.io.tmpdir=/home/aiuap/tmp
 
select TO_TIMESTAMP(substr(replace(t.timereported,'T',' '),0,19),'yyyy-mm-dd hh24:mi:ss.ff') from IAP_LINUX_SYSLOG t where t.timereported like '2019%';
查询服务线程排序
ps -o nlwp,pid,lwp,args -u aiuap|sort -n

keytool -genkeypair -alias asia -keyalg RSA -validity 36500 -storepass 'ADadmin12!@' -keystore ./test.keystore -v
keytool -genkeypair -alias audit -keyalg RSA -validity 36500 -keystore ./test.keystore -v
keytool -genkeypair -alias 10.1.198.53:6030 -keyalg RSA -validity 36500 -keystore ./test.keystore -v
keytool -genkeypair -alias 10.1.198.53 -keyalg RSA -validity 36500 -keystore ./test.keystore -v
keytool -genkeypair -alias xxxxxx -keyalg RSA -validity 36500 -keystore ./test.keystore -v
 
keytool -exportcert -alias asia -file ./asia.cer -storepass 'ADadmin12!@' -keystore ./test.keystore -v
keytool -exportcert -alias audit -file ./audit.cer -storepass 'ADadmin12!@' -keystore ./test.keystore -v
keytool -exportcert -alias 10.1.198.53:6030 -file ./10.1.198.53:6030.cer -storepass 'ADadmin12!@' -keystore ./test.keystore -v
keytool -exportcert -alias 10.1.198.53 -file ./10.1.198.53.cer -storepass 'ADadmin12!@' -keystore ./test.keystore -v
keytool -exportcert -alias xxxxxx -file ./xxxxxx.cer -storepass 'ADadmin12!@' -keystore ./test.keystore -v

keytool -list -keystore ./test.keystore 
keytool -list -keystore ./.keystore 

keytool -genkeypair -alias asia -keyalg RSA -validity 36500 -storepass 'ADadmin12!@' -keystore keystore.jks   -v



keytool -genkeypair -alias iap -keyalg RSA -validity 36500 -storepass 'ADadmin12!@' -keystore ./iap.keystore -v
keytool -genkeypair -alias report -keyalg RSA -validity 36500 -storepass 'ADadmin12!@' -keystore ./report.keystore -v
keytool -genkeypair -alias dac -keyalg RSA -validity 36500 -storepass 'ADadmin12!@' -keystore ./dac.keystore -v
修改zookeeper里面的数据
set /ausApp/web/dataprocess/tcp-xml/config.json {"host_Ip":"192.168.90.25","host_User":"lccy","runtime_memory":"2048","main.thread.num":"1","main.bitch.size":"5000","main.alarm.serverip":"127.0.0.1","main.master":"true","main.exception.exit":"false","read.event.general.interface":"com.ai.aus.dataprocess.kafka.GeneralEventFilter","read.characterSet":"utf-8","read.zookeeper.sync.time.ms":"1000","read.consumer.timeout.ms":"30000","read.group.id":"aus-group","read.topic":"aus","source.replication-factor":"1","source.partition":"1"}
												 																						 
set /ausApp/web/dataprocess/iap_linux_syslog/config.json {"host_Ip":"192.168.90.25","host_User":"lccy","runtime_memory":"2048","main.thread.num":"1","main.bitch.size":"5000","main.alarm.serverip":"127.0.0.1","main.master":"true","main.exception.exit":"false","read.event.general.interface":"com.ai.aus.dataprocess.kafka.GeneralEventFilter","read.characterSet":"utf-8","read.zookeeper.sync.time.ms":"1000","read.consumer.timeout.ms":"30000","read.group.id":"linuxsyslog","read.topic":"linuxsyslog","source.replication-factor":"1","source.partition":"1"}


project = SSSIA AND issuetype = 故障 AND status in (Open, 已上线, 已发布, 待发布, 待测试, 测试中, 定位中, 待修复, 故障修复中) AND assignee in (wangsz, tanrq, rensy, chenxiang5, chendd, sunqc5, zhangjk, lizp, lusm, jingll,zhanghy18)
sed -i 's/http:\/\/.*\/iap/http:\/\/10.1.198.53:9030\/iap/g' `find local config -type f|xargs grep '"IAP'|awk -F: '{print $1}'`


for i in 268 273 264 228 229 231 265 266 267 269 270 271 272 274 275 276 277 100040 100300 999999 ; do sed "s/ORGID/$i/g" orgid.sql >> qqqqqqq.sql; done

for i in 201908 201909 201910 201911 201912 ; do sed "s/YYYYMM/$i/g" qqqqqqq.sql >> xxxx.sql; done
#linux合并行
sed ':a;N;$!ba;s/\n//g'  acct.txt 
sed '/^<LOG4A>$/{:a;N;/<\/LOG4A>/!ba;s/\n//g}' BOSS111_4.XML 
#按照标签拆分行
sed ':a;N;$!ba;s/<\/\([0-9a-zA-Z_]*\)>/<\/\1>\n/g' BOMC_Audit_2019-11-13-23-00.xml
删除空行
sed -i /^[[:space:]]*$/d   xxxxxxxx.dat 

grep -E '<.*>.*<.*>' BOSS30_1.XML  |awk -F '<|>' '{print $2" "$3}' > testdata
grep -E '<.*>.*<.*>' BOSS30_1.XML  |awk -F '<|>' -v OFS='|' '{print $2,$3}' > testdata
./pulsar-admin topics peek-messages -s hah -n 100 persistent://test/test/boss-ftp-flume-app|grep 'fileName'|awk '!a[$0]++{print}'
./pulsar-admin topics peek-messages -s hah -n 100 persistent://test/test/boss-ftp-flume-app|grep 'fileName'|awk '{a[$0]++}END{for(i in a){print i}}'
#windows获取cpu序列号
wmic cpu get processorid
#linux挂载iso镜像
mount -o loop ubuntu-18.04.2-live-server-amd64.iso /mnt/


Downloading: http://nexus.pentaho.org/content/groups/omni/pentaho/pentaho-big-data-plugin/8.0.0.0-28/pentaho-big-data-plugin-8.0.0.0-28.zip

mvn install:install-file -DgroupId=pentaho -DartifactId=pentaho-big-data-plugin -Dversion=8.0.0.0-28 -Dpackaging=zip -Dfile=pentaho-big-data-plugin-8.0.0.0-28.zip

Downloading: http://nexus.pentaho.org/content/groups/omni/pentaho/pentaho-karaf-assembly/8.0.0.0-28/pentaho-karaf-assembly-8.0.0.0-28-client.zip

mvn install:install-file -DgroupId=pentaho -DartifactId=pentaho-karaf-assembly -Dversion=8.0.0.0-28 -Dclassifier=client -Dpackaging=zip -Dfile=pentaho-karaf-assembly-8.0.0.0-28-client.zip


jdbc-16.jar

mvn install:install-file -Dfile=jdbc-16.jar -DgroupId=com.dameng -DartifactId=jdbc -Dversion=16 -Dpackaging=jar
mvn package -Dmaven.test.skip=true
mvn install:install-file -Dfile=3des.jar -DgroupId=com.asiainfo -DartifactId=3des -Dversion=1.1 -Dpackaging=jar

#linux远程执行命令
expect -c "
spawn ssh aiiap@10.21.171.47  ls
expect {
    \"*assword:\" 
                {
                    set timeout 300; 
                    send \"aiiap\r\";
                }
				}
expect eof"


expect -c "
spawn ssh aiiap@10.21.171.47  ls
expect \"*assword:\" {set timeout 300;  send \"aiiap\r\";}
expect eof"

# 查询命令所在的安装包
yum search netstat
#对数据的时间统一改造
update iap_ori_dev_session_cmd_201911 t set t.ori_create_time = t.ori_create_time - rownum*5/1440
#标准化日志报错去重
grep ERROR err.log |awk '{a[$5]=$0}END{for(i in a){print a[i]}}'|sort

#oracle导入dmp文件
imp audit30testa/audit30testa#@iap file = ori_cmd_201911.dmp log = ori_cmd_201911.log full = y

cd /opt/pulsar/pulsar-2.4.0/bin/
./pulsar-client consume -s hah persistent://test/test/boss-ftp-flume-app
./pulsar-client consume -s rensy persistent://test/test/canalSinkPulsar-sunqc
./pulsar-client consume -s hah persistent://test/test/boss-ftp-ori-app
./pulsar-client consume -s hah persistent://test/test/boss-ftp-errlog-app
./pulsar-client consume -s hah persistent://test/test/boss-ftp-exception-app
./pulsar-client produce -m asdasdfasdf persistent://test/test/boss-ftp-flume-app

./pulsar-client produce -m '{"SERVER_PORT":"8020","fileName":"BASS_AUDIT_2020-05-13-17-23-53-hhhtsjzx-zhhbase-master-212-30.COMPLETED","RESOURCE_CODE":"JTNGSYHUZHJQS","SUB_ACCOUNT_NAME":"hbase","CLIENT_ADDRESS":"10.252.216.143","OP_TYPE_NAME":"rpc","RESOURCE_KIND":"1","SERVER_ADDRESS":"10.252.212.30","filePath":"/data/aiiap/rensy/new","IDENTITY_NAME":"JTNGSYHUZHJQS","OP_TYPE_ID":"1-JTNGSYHUZHJQS-10001","OPERATE_TIME":"2020-04-28 18:37:02","OBJECT_ID":"1","OPERATE_CONTENT":"HDFSAudit_getfileinfo","IDR_CREATION_TIME":"2020-05-13 17:26:20","OBJECT_NAME":"/apps/hbase/data/data/szfms/Ht_ALL_CDR_RULE_SINGLE/71003834c088c143cb3c85a6fbb37aab/c/b69a0b2e8f94464086f86f700ad1153d","OPERATE_RESULT":"1","gatherTime":"1590063065479","timestamp":"1590063065417"}' persistent://test/test/par-rensy-tcp

./pulsar-admin topics stats-internal persistent://test/test/boss-ftp-flume-app
./pulsar-admin topics delete persistent://test/test/canalSinkPulsar-test513
./pulsar-admin topics delete persistent://test/test/boss-ftp-flume-app
./pulsar-admin topics create persistent://test/test/boss-ftp-ori-app
./pulsar-admin topics create persistent://test/test/boss-ftp-errlog-app
./pulsar-admin topics create persistent://test/test/boss-ftp-exception-app
iconv -f utf-8 -t gb2312 /server_test/reports/software_.txt > /server_test/reports/software_asserts.txt
./pulsar-admin persistent stats persistent://test/test/boss-ftp-ori-app
./pulsar-admin persistent stats persistent://test/test/boss-ftp-flume-app
./pulsar-admin topics stats persistent://test/test/boss-ftp-ori-app
./pulsar-admin topics stats persistent://test/test/boss-ftp-errlog-app
./pulsar-admin topics stats persistent://test/test/boss-ftp-exception-app
./pulsar-admin topics peek-messages -s hah -n 100 persistent://test/test/boss-ftp-flume-app|grep 'fileName'


./pulsar-admin topics create persistent://test/test/boss-ftp-flume-boss
./pulsar-admin topics create persistent://test/test/boss-ftp-ori-boss-log
./pulsar-admin topics create persistent://test/test/boss-ftp-ori-boss-errlog
./pulsar-admin topics create persistent://test/test/boss-ftp-ori-boss-exception
./pulsar-admin topics create persistent://test/test/boss-ftp-mysql-boss-errlog
./pulsar-admin topics create persistent://test/test/boss-ftp-mysql-boss-exception

./pulsar-client consume -s flume-rensy persistent://test/test/flume-rensy-tcp -n 100
./pulsar-client consume -s par-rensy persistent://test/test/par-rensy-tcp -n 100
./pulsar-client consume -s par-diea-rensy persistent://test/test/par-diea-rensy -n 100
./pulsar-client consume -s par-error-rensy persistent://test/test/par-error-rensy -n 100

./pulsar-client consume -s par-unIdentity-rensy persistent://test/test/unIdentityNameTopic-rensy -n 100


./pulsar-client consume -s UnInsert-rensy persistent://test/test/apporiUnInsertTopic-rensy -n 100


./pulsar-client consume -s std-rensy persistent://test/test/apps718-rensy -n 100
./pulsar-client consume -s std-gold-rensy persistent://test/test/appgold-rensy -n 100
./pulsar-client consume -s std-round-rensy persistent://test/test/appround-rensy -n 100
./pulsar-client consume -s std-fail-rensy persistent://test/test/standardizeFailTopic-rensy -n 100
./pulsar-client consume -s std-error-rensy persistent://test/test/errorTopic-rensy  -n 100

bin/pulsar-admin topics list test/test

persistent://test/test/canalSinkPulsar-error513
./pulsar-client consume -s diea-rensy persistent://test/test/canalSinkPulsar-diea513 -n 100

./pulsar-client consume -s rensy persistent://test/test/canalSinkPulsar-test513 -n 10000|grep uap_main_acct|awk -F'uap_main_acct' '{print NF}'
./pulsar-client consume -s rensy persistent://test/test/canalSinkPulsar-test513 -n 2|grep uap_main_acct|awk '{print "\{"$0}'|jq .
./pulsar-client consume -s rensy persistent://test/test/canalSinkPulsar-test513 -n 2|grep uap_main_acct|awk '{print "\{"$0}'
./pulsar-client consume -s std-rensy1 persistent://test/test/apps718-rensy -n 100 |grep CTBOSS|jq .AUDIT_LEVEL_ID,.AUDIT_TYPE_ID,.ACTION_TYPE_ID,.OBJ_TYPE_ID,.APP_IMPORT_LEVEL,.LEVEL2_ORG_ID,.CLIENT_IP_SECTION_ID,.CLIENT_IP_SECTION_NAME,.SERVER_IP_SECTION_ID,.SERVER_IP_SECTION_NAME
./pulsar-client consume -s par-unIdentity-rensy persistent://test/test/unIdentityNameTopic-rensy -n 100000|grep IDENTITY_NAME|awk '{a[$0]++}END{for(i in a){print i}}
grep -Eo '<OPERATE_TIME>2019-.*</OPERATE_TIME>' *|awk -F'COMPLETED:' '{print $2}'|awk -F'>| ' '{print $2}'|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'
grep -Eo '<OPERATE_TIME>.*</OPERATE_TIME>' *|awk -F'COMPLETED:' '{print $2}'|awk -F'>| ' '{print $2}'|awk -F'-' '{print $1"-"$2}'|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'

grep -Po '<OPERATE_TIME>.*?</OPERATE_TIME>' *.COMPLETED|awk -F'COMPLETED:' '{print $2}'|awk -F'>| ' '{print $2}'|awk -F'-' '{print $1"-"$2}'|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'

grep -Po '<IDENTITY_NAME>.*?</IDENTITY_NAME>' *.COMPLETED|awk -F'<|>' '{a[$3]++}END{for(i in a){print i" "a[i]}}'

for i in `grep '&' *.COMPLETED|awk -F: '{print $1}'|awk '{a[$1]++}END{for(i in a){print i}}'` ;do mv $i $i.geshi;done
for i in `grep '</OPERATE_RESULT>成功' *.COMPLETED|awk -F: '{print $1}'|awk '{a[$1]++}END{for(i in a){print i}}'` ;do mv $i $i.chenggong;done
for i in `grep 'JTNGDQAP4' *.COMPLETED|awk -F: '{print $1}'|awk '{a[$1]++}END{for(i in a){print i}}'` ;do mv $i $i.JTNGDQAP4;done
for i in `grep '</ROOT><' *.COMPLETED|awk -F: '{print $1}'|awk '{a[$1]++}END{for(i in a){print i}}'` ;do mv $i $i.root;done
for i in `grep -Po '<OPERATE_TIME>.*?</OPERATE_TIME>' *.COMPLETED|awk -F: 'length($2)>27{print $1}'|awk '{a[$1]++}END{for(i in a){print i}}'` ;do mv $i $i.shijian;done
xmllint  *.*>/dev/null 2>h || for i in `awk -F: '{print $1}' h|grep COMPLETED|awk '{a[$0]++}END{for(i in a){print i}}'` ;do mv $i $i.geshi;done;rm h
for i in `ls *.COMPLETED` ;do mv $i ${i/.COMPLETED/};done


for i in `bin/pulsar-admin topics list 4a/audit`;do echo $i;bin/pulsar-admin topics info-internal $i|jq .ledgers[].size|grep -v null|awk '{a+=$1}END{print a}';done >log2


grep -Po '<OPERATE_TIME>.*?</OPERATE_TIME>' *.COMPLETED|awk -F'COMPLETED:' '{print $2}'|awk -F'>| ' '{print $2}'|awk -F'-' '{print $1"-"$2}'|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'
grep -Po '<OPERATE_TIME>.*?</OPERATE_TIME>' *.COMPLETED|wc -l

./redis-cli --raw -h 10.21.171.181 -p 6379 keys *rrrrr*
./redis-cli --raw -h 10.21.171.181 -p 6379 get main.acct.name.namespace:MAINACCTNAME_rrrrr12|base64 -d|awk -F'|' '{print $0}'
cmdkey='keys *qianfang*' && ./redis-cli -h 10.21.171.181 -p 6379 $cmdkey && ./redis-cli -h 10.21.171.181 -p 6380 $cmdkey && ./redis-cli -h 10.21.171.182 -p 6379 $cmdkey && ./redis-cli -h 10.21.171.182 -p 6380 $cmdkey && ./redis-cli -h 10.21.171.183 -p 6379 $cmdkey && ./redis-cli -h 10.21.171.183 -p 6380 $cmdkey
for i in `./redis-cli --raw -h 10.21.171.181 -p 6379 keys app.subacct.author*` ;do ./redis-cli --raw -h 10.21.171.181 -p 6379 ttl $i;done|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'



sed -e 's/<?xml version="1.0" encoding="UTF-8"?><ROOT>//g' -e 's/<\/ROOT>//g' -e 's/IDLE//g'20200520/* >xxxxxxxx.dat 

./redis-cli --raw -h 10.21.171.181 -p 6379 flushall

du -sb *|awk '$1<100{print $2}'|xargs cat
xmllint  *.*>/dev/null 2>h || awk -F: '{print $1}' h|grep COMPLETED ;rm h

xmllint  *.*>/dev/null 2>h || awk -F: '{print $1}' h|grep COMPLETED.and|awk '{a[$0]++}END{for(i in a){print i}}'|wc -l ;rm h
xmllint  *.*>/dev/null 2>h || for i in `awk -F: '{print $1}' h|grep COMPLETED.and|awk '{a[$0]++}END{for(i in a){print i}}'` ;do mv $i $i.geshi;done;rm h

bin/bookkeeper shell simpletest --ensemble 3 --writeQuorum 3 --ackQuorum 3 --numEntries 3
./pulsar-daemon start zookeeper
./pulsar-daemon start broker
./pulsar-daemon start bookie
./pulsar-daemon stop broker
./pulsar-daemon stop bookie
./pulsar-daemon stop zookeeper
ps -ef |grep -E 'broker|bookie'


bin/pulsar-admin functions create \
--jar test/xml-parse-function-1.0-SNAPSHOT-jar-with-dependencies.jar \
--classname com.asiainfo.dap.XmlParseFunction \
--inputs persistent://test/test/boss-ftp-flume-boss \
--output persistent://test/test/boss-ftp-ori-boss-log \
--user-config '{"errorTopic":"persistent://test/test/boss-ftp-ori-boss-errlog","exceptionTopic":"persistent://test/test/boss-ftp-ori-boss-exception","record-tag-name":"LOG4A"}' \
--tenant test \
--namespace test \
--name xml-parse \
--processing-guarantees EFFECTIVELY_ONCE \
--cpu 4 \
--ram 1073741824 \
--disk 68719476736 \
--parallelism 1


bin/pulsar-admin functions update \
--jar test/xml-parse-function-1.0-SNAPSHOT-jar-with-dependencies.jar \
--classname com.asiainfo.dap.XmlParseFunction \
--inputs persistent://test/test/boss-ftp-flume-boss \
--output persistent://test/test/boss-ftp-ori-boss-log \
--user-config '{"errorTopic":"persistent://test/test/boss-ftp-ori-boss-errlog","exceptionTopic":"persistent://test/test/boss-ftp-ori-boss-exception","record-tag-name":"LOG4A"}' \
--tenant test \
--namespace test \
--name xml-parse \
--processing-guarantees EFFECTIVELY_ONCE \
--cpu 4 \
--ram 1073741824 \
--disk 68719476736 \
--parallelism 6


bin/pulsar-admin sinks create \
--classname com.asiainfo.dap.JdbcAutoSchemaSink \
--archive test/pulsar-mysql-sink-2.4.0.nar \
--inputs persistent://test/test/boss-ftp-ori-boss-log \
--tenant test \
--namespace test \
--name test-mysql-sink \
--sink-config-file test/conf/pulsar-mysql-sink/pulsar-mysql-sink-two.yaml \
--cpu 1 \
--ram 2147483648 \
--disk 25769803776 \
--parallelism 1

./pulsar-admin functions delete --name xml-parse --tenant test --namespace test
./pulsar-admin sinks delete --name test-mysql-sink --tenant test --namespace test





cd /opt/ &&
cat /etc/redhat-release &&
echo '---------------------------------------------------------------' &&
cat /proc/version &&
echo '---------------------------------------------------------------' &&
awk -F: '{a[$1]=$0}END{for(i in a){print a[i]}}' /proc/cpuinfo |grep -E 'processor|model name' &&
echo '---------------------------------------------------------------' &&
free -g &&
echo '---------------------------------------------------------------' &&
dd if=/dev/zero bs=1024 count=1000000 of=1Gb.file &&
echo '---------------------------------------------------------------' &&
dd if=1Gb.file bs=64k |dd of=/dev/null &&
echo '---------------------------------------------------------------' &&
rm -f 1Gb.file &&
ping 10.21.171.60 -c 10|tail -n 1
echo '---------------------------------------------------------------' &&
iperf -c 10.21.171.60




sed -i 's/audit30dev1/audit30testa/g' `find . -type f |grep .ktr`
sed -i 's/AUDIT30DEV1/audit30testa/g' `find . -type f |grep .ktr`
sed -i 's/Encrypted 2be98afc86ac68780a20dfd20da97b98b/Encrypted 2be98afa91fc39b90f849ba75cd86ae99/g' `find . -type f |grep .ktr`
sed -i 's/10.15.42.37/10.21.171.220/g' `find . -type f |grep .ktr`
sed -i 's/10.15.42.37/10.21.171.220/g' `find . -type f |grep -E 'c3p0.properties|business.properties|application.yml'`

sed -i 's/10.15.42.37/10.21.171.220/g' `find . -type f |grep -E '.properties'`

mysqldump -u iap -h 127.0.0.1 -p --databases iap iap_dev > backdb.sql
mysql -u iap -p  < backdb.sql

MYSQL
select * from information_schema.columns where table_schema='iap' and column_comment <>''

select distinct COLUMN_NAME,column_comment  from information_schema.columns where table_schema='iap' and column_comment <>'' ORDER BY COLUMN_NAME

select CONCAT('alter table ',TABLE_NAME,' modify column ',COLUMN_NAME,' int comment ','\'\';') from information_schema.columns where table_schema='iap' and column_comment ='' order by COLUMN_NAME

select CONCAT('alter table ',TABLE_NAME,' modify column ',COLUMN_NAME,' int comment ','\'\';') from information_schema.columns where table_schema='iap' and column_comment ='' and TABLE_NAME not like '%20%' and TABLE_NAME not like '%yyyymm%' order by COLUMN_NAME

select CONCAT('alter table ',TABLE_NAME,' modify column ',COLUMN_NAME,' int comment ','\'\';') from information_schema.columns where table_schema='iap' and column_comment ='' and TABLE_NAME  like '%202001%' order by COLUMN_NAME


select count(*) from information_schema.columns where table_schema='iap' and column_comment ='' and TABLE_NAME not like '%20%';
select count(*) from information_schema.columns where table_schema='iap' and column_comment ='' and TABLE_NAME  like '%202001%';
alter table test1 modify column field_name int comment '修改后的字段注释';

ORACLE
select 'comment on column "' || a.table_name || '"."' || a.column_name ||
       '" is ''' || b.comments || ''';'
  from (select *
          from user_col_comments d
         where (d.comments is null or d.comments like '%?%')
           and d.table_name not like '%=%'
           and d.table_name not like '%20%') a
  left join (select t.column_name, max(t.comments) as comments
               from user_col_comments t
              where t.comments is not null
                and t.comments not like '%?%'
                and t.table_name not like '%=%'
                and t.table_name not like '%20%'
              group by t.column_name) b
    on a.column_name = b.column_name;
    
select 'comment on column "' || a.table_name || '"."' || a.column_name ||
       '" is ''' || b.comments || ''';'
  from (select *
          from user_col_comments d
         where (d.comments is null or d.comments like '%?%')
           and d.table_name not like '%=%'
           and d.table_name not like '%20%') a
  left join (select t.column_name, max(t.comments) as comments from user_col_comments@uapdblink t
              where t.comments is not null
                and t.comments not like '%?%'
                and t.table_name not like '%=%'
                and t.table_name not like '%20%'
              group by t.column_name) b
    on a.column_name = b.column_name;
	
select 'comment on table ' || a.table_name || '  is  ''' || b.table_comment ||
       ''';'
  from (select *
          from user_tab_comments t
         where t.table_type = 'TABLE'
           and (t.comments is null or t.comments like '%?%')
           and t.table_name not like '%20%') a
  join mysql_table_comments b
    on a.table_name = upper(b.table_name);

select dbms_pipe.receive_message('RDS', 10) from dual;


netstat -anop |grep ' LISTEN ' |awk '{print $4"  "$7" "system("pwdx "substr($7,0,index($7,"/")-1))}'

curl -u 'rensy:3edc$RFV' https://jira.asiainfo-sec.com:8443/rest/api/2/issue/SSSIA-3153 |jq .
curl -u 'rensy:3edc$RFV' https://jira.asiainfo-sec.com:8443/rest/api/2/issue/SSSIA-3240/comment
curl -D- -u 'rensy:3edc$RFV' -X POST --data '{    "fields": {       "project":       {           "key": "SSSIA"       },"components": [{"id":"10602"}], "customfield_10007":[{"id":"11440"}], "duedate":"2020-08-05","assignee":{"name": "rensy"},      "summary": "REST ye merry gentlemen.",       "description": "Creating of an issue using project keys and issue type names using the REST API",       "issuetype": {          "name": "BUG"       }   }}' -H "Content-Type: application/json" https://jira.asiainfo-sec.com:8443/rest/api/2/issue/
jps -q|awk '{cmd="pstree -p "$1"|wc -l";print $1,system(cmd)}'|sed '$!N;s/\n/ /g'
lsof -P |awk '{a[$2]++}END{for(i in a){print i" "a[i]}}'

while true; do sleep 1;nc 10.21.171.181 29290 < ./wyd/log_test_new.txt ; done
./pulsar-admin topics subscriptions persistent://4a/audit/parse_devlog_hndxtest
./pulsar-admin topics unsubscribe -s 'shared-subscription-test' persistent://4a/audit/parse_devlog_hndxtest
while true do;nc 10.21.171.181 29290 < ./wyd/log_test_new.txt ; done


for i in /tmp/*; do echo $i; find $i |wc -l|sort -nr; done
awk  '$1=="Count:"{print $0}' all.txt |awk -F'[()]' '{print $2}'|sed 's/s//g'|awk '$1>5000{print $0}'|sort -n
awk  '$1=="Count:"{print $3}' all.txt |sed -e 's/Time=//g' -e 's/s//g'|awk '$1>100{print $0}'|sort -n
awk  '$1=="Count:"{print $3}' all.txt |sed -e 's/Time=//g' -e 's/s//g'|awk '$1>500{print $0}'|sort -n
awk  '$1=="Count:"{print $5}' all.txt |sed -e 's/Lock=//g' -e 's/s//g'|awk '$1>1{print $0}'|sort -n
awk  '$1=="Count:"{print $6}' all.txt |sed -e 's/(//g' -e 's/s)//g'|awk '$1>1{print $0}'|sort -n
import requests
s = requests.session()
s.auth = ('用户名', '密码')
r = s.get("URL")
print(r.text)
@renshunyu

def getIssue(id):
    URL = 'https://jira.asiainfo-sec.com:8443/rest/api/2/issue/'
    s = requests.session()
    s.auth = ('rensy', '3edc$RFV')
    r = s.get(URL+id)
    return r.text
	
sed -n 'H;${G;s/\r\n/ /g;s/\n/ /g;p}' test|wc -l
sed -n 'H;${x;s/\r\n/ /g;s/\n/ /g;p}' test|wc -l
sed -n 'H;1!{/^# Time:/{x;s/\r\n/ /g;s/\n/ /g;p}}' slow.log  >test1
sed 's/localhost \[\]/\[localhost\]/g' test1|sort -nr -k13|head -n 10

i=`cat test`&&j=`echo $i|sed 's/ /p;/g'`"p"&&echo $j|sed -n $j top10.txt
sed 's/localhost \[\]/\[localhost\]/g' test1|sort -nr -k13|head -n 10
awk '{print $13" "NR}' test1|sort -nr -k1
awk '{if($13!="Query_time:"){print $13" "NR}else{print $14" "NR}}' test1|sort -nr -k1|head -n 10|awk '{print $2}'
i=`awk '{if($13!="Query_time:"){print $13" "NR}else{print $14" "NR}}' test1|sort -nr -k1|head -n 10|awk '{print $2}'`&&j=`echo $i|sed 's/ /p;/g'`"p"&&echo $j|sed -n $j test1

pstree -p PID

sed -n 'H;1!{/^# Time:/{x;s/\r\n/ /g;s/\n/ /g;p}}' slow.log >test1 
sed 's/localhost \[\]/\[localhost\]/g' test1|sort -nr -k13|head -n 10 
awk '{if(FNR==1){print $0}}' *|awk '{a[$0]++}END{for(i in a){print i" "a[i]}}'

sed -n '1H;1!{/^comment/{x;/YYYYMM/p};/^comment/!{H}}' allcomments.sql >YYYYMMANDORGID.sql
sed -n '1H;1!{/^comment/{x;/YYYYMM/!p};/^comment/!{H}}' allcomments.sql >NOMONTH.sql
sed -n '1H;1!{/^comment/{x;/ORGID/!p};/^comment/!{H}}' YYYYMMANDORGID.sql >YYYYMM.sql
sed -n '1H;1!{/^comment/{x;/ORGID/p};/^comment/!{H}}' YYYYMMANDORGID.sql >ORGID.sql
grep -E 'CREATE|create' rsdu.sql|grep -E 'TABLE|table'|awk '{print $3}'|sed 's/"//g'
sed  ':a;N;s/\n/\\|/g;ba' mssb.sql
sed -n '1H;1!{/^comment/{x;/IAP_DEV_MIDDLE_RESULT\|IAP_DATA_UPLOAD_SCHEDULE\|IAP_DEV_SENSITIVE_RESULT\|IAP_DB_SENSITIVE_MAPPING\|IAP_HOST_SENSITIVE_MAPPING\|IAP_APP_SENSITIVE_RESULT\|IAP_APP_SENSITIVE_MAPPING\|DATA_UPLOAD_RESULT\|IAP_DATA_UPLOAD_CHECK_RESULT\|IAP_NET_DATA_UPLOAD_SCHEDULE\|IAP_NET_DB_SENSITIVE_MAPPING\|IAP_NET_HOST_SENSITIVE_MAPPING\|IAP_NET_APP_SENSITIVE_RESULT\|IAP_NET_APP_SENSITIVE_MAPPING\|IAP_NET_DEV_SENSITIVE_RESULT\|DATA_NET_UPLOAD_RESULT\|IAP_NET_DATA_UPLOAD_RESULT\|IAP_NET_DEV_MIDDLE_RESULT\|IAP_DATA_UPLOAD_SCHEDULE_NXNET\|IAP_APP_SENSITIVE_MAPPING_NX\|IAP_APP_SENSITIVE_RESULT_NX\|DATA_REPORT_RESULTS\|SENSITIVE_PERSONNEL_LIST\|IAP_DEV_SENSITIVE_MAPPING_NX\|ZYONLINE_REPORT_FILE\|ZYONLINE_REPORT_INCREASE\|IAP_SENSITIVE_OPERLOG_RESULT\|DR_OPERATECONTENT_CHARSET\|DR_MAINACCT_INCREASE_DAY\|DR_4A_LOGINAPP_DAY\|DR_COMPLAIN_MONTH\|DATA_UPLOAD_TASK_INFO\|IAP_DATA_UPLOAD_PROCESS\|DR_2_4A_APP_SUB_ROLE_DAY\|DR_2_4A_LOGIN_APP_DAY\|DR_2_4A_LOGIN_DAY\|DR_2_4A_MAIN_ACCT_DAY\|DR_2_4A_MAIN_ACCT_MONTH\|DR_2_APP_ACCT_DAY\|DR_2_APP_ACCT_MONTH\|DR_2_DEV_SUB_ACCT_MONTH\|DR_2_GOLD_DAY\|DR_2_MAIN_SUB_ACCT_MONTH\|DR_2_NO_GOLD_DAY\|DR_2_PERSON_DAY\|DR_2_PERSON_MONTH\|DR_COMM_4A_APP_ROLE_MONTH\|DR_COMM_4A_APP_RP_MINUTE\|DR_COMM_4A_PERMISSION_MONTH\|DR_COMM_ASSET_INFOR_MONTH\|DR_COMM_GOLD_APPLY_DAY\|DR_COMM_GOLD_APPROVAL_DAY\|DR_COMM_INTERFACE_CALL_DAY\|DR_COMM_SENSI_DATA_ENCOD_DAY\|DR_COMM_SENSITIVE_OP_WO_DAY\|DR_IT_4A_APP_ACCT_MINUTE\|DR_IT_4A_APP_ACCT_MONTH\|DR_IT_4A_APP_SUB_ROLE_MINUTE\|DR_IT_4A_LOGIN_APP_MINUTE\|DR_IT_4A_LOGIN_MINUTE\|DR_IT_4A_MAIN_ACCT_MINUTE\|DR_IT_4A_MAIN_ACCT_MONTH\|DR_IT_AUDIT_REPORT_DAY\|DR_IT_DEV_SUB_ACCT_MONTH\|DR_IT_GOLD_MINUTE\|DR_IT_MAIN_SUB_ACCT_MONTH\|DR_IT_NO_GOLD_MINUTE\|DR_IT_SENSITIVE_PERSON_MINUTE\|DR_IT_SENSITIVE_PERSON_MONTH\|DATA_UPLOAD_TASK_SCHEDULE\|DR_MONITOR_RESULT_DOWNLOAD\|DR_MONITOR_LAST_TASK_INFO/!p};/^comment/!{H}}' NOMONTH.sql >NORSD.sql


sed -e 's/iap\.jdbc\.password=.*/iap\.jdbc\.password=16|-98|-1|-3|122|-48|-50|59|127|121|54|-32|63|109|-24|-56|5/g' -e 's/iap\.jdbc\.username=.*/iap\.jdbc\.username=audit30testa/g' -e 's/iap\.url\.server=.*/iap\.url\.server=jdbc:oracle:thin:@10\.21\.171\.220:1521:iap/g' `find . -name env.properties`

curl -c cookies http://10.21.171.240:8088/zentao/?mode=getconfig
curl -c cookies http://10.21.171.240:8088/zentao/api-getSessionID.json
curl -i -X POST -b cookies -H "'Content-Type': 'application/x-www-form-urlencoded'"  -d 'account=admin&password=1qaz!QAZ' http://10.21.171.240:8088/zentao/user-login
curl -i -X POST 'http://10.21.171.240:8088/zentao/testcase-browse-1--byModule-all-id_asc-0-10000-1.json?zentaosid=8a49b1o8mf24113duvfhblipt6'|grep status |jq .data|sed -e 's/\\\\/\\/g' -e 's/\\"/"/g' -e 's/^"//g' -e 's/"$//g'|jq .cases |jq 'keys'|jq .[]
#linux对比目录
diff 1 2 -urNaq
#ASCII转汉字
native2ascii [ inputfile ] [ outputfile ]

grep '' `find . -name pom.xml` |sed -n '/fastjson/{N;p}'
date "+%S %M %H * * ?" -d '20 second'
svn log -r 178174:177855

for i in `svn log -r 178174 --verbose|grep 'products'|awk -F'/products/ailkiap30/program/iap/' '{print $2}'` ;do svn log -r178174:177855  $i ; done|grep -E '^r'

find . -type f -size +1G

